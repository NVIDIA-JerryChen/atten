"""
demo_fa4.py - FA4 Block Sparse Attention Demo for B200 (Blackwell/SM100)

å¯¹æ¯”æµ‹è¯•:
- FA4 Block Sparse Attention (flash_attn.cute.interface)
- PyTorch SDPA (dense)
- FlexAttention (sparse)

æµ‹è¯• Shape:
- head_dim=64: [8, 130560, 4, 64], [64, 65280, 2, 64]
- head_dim=128: [2, 1590, 4, 128], [2, 8160, 4, 128]

Usage:
    python demo_fa4.py --device cuda:0 --sparsity 0.5
    python demo_fa4.py --accuracy-only
    python demo_fa4.py --performance-only --log
"""

import sys
import os

# æŠŠç³»ç»ŸåŒ…è·¯å¾„æ”¾æœ€å‰é¢
# sys.path.insert(0, '/usr/local/lib/python3.12/dist-packages')

import argparse
from termcolor import colored
import torch
import torch.nn.functional as F
import math
from typing import Optional, Tuple, Callable
from datetime import datetime
import sys
import nvtx
import pandas as pd
from torch.nn.attention.flex_attention import (
    flex_attention,
    create_block_mask,
    BlockMask,
)

# FA4 cute interface
# try:
from flash_attn.cute.interface import _flash_attn_fwd
from flash_attn.cute.block_sparsity import BlockSparseTensorsTorch 
from flash_attn.cute import utils as cute_utils
import cutlass
import cutlass.cute as cute
FA4_AVAILABLE = True
# except ImportError as e:
#     FA4_AVAILABLE = False
#     FA4_IMPORT_ERROR = str(e)
#     print(f"Warning: FA4 cute interface not available: {e}")
#     # å®šä¹‰å ä½ç±»å‹ï¼Œé¿å…ç±»å‹æ³¨è§£æŠ¥é”™
#     from typing import NamedTuple
#     class BlockSparseTensorsTorch(NamedTuple):
#         mask_block_cnt: torch.Tensor
#         mask_block_idx: torch.Tensor
#         full_block_cnt: Optional[torch.Tensor] = None
#         full_block_idx: Optional[torch.Tensor] = None
    
#     def fast_sampling(fn):
#         return fn

# é¢„ç¼–è¯‘ flex_attention
torch._dynamo.reset()
flex_attention_compiled = torch.compile(
    flex_attention, dynamic=False, mode="max-autotune-no-cudagraphs"
)

WARMUP_ITERATIONS = 10
PERF_BENCHMARK_ITERATIONS = 20
BLOCK_USER = 128


class TeeLogger:
    """å°†è¾“å‡ºåŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
    
    def __init__(self, log_file_path: str):
        self.terminal = sys.stdout
        self.log_file = open(log_file_path, 'w', encoding='utf-8')
        self.closed = False
    
    def write(self, message: str) -> None:
        self.terminal.write(message)
        if not self.closed:
            try:
                self.log_file.write(message)
                self.log_file.flush()
            except (OSError, ValueError):
                self.closed = True
    
    def flush(self) -> None:
        self.terminal.flush()
        if not self.closed:
            try:
                self.log_file.flush()
            except (OSError, ValueError):
                self.closed = True
    
    def close(self) -> None:
        self.closed = True
        if self.log_file:
            self.log_file.close()


def generate_block_mask_128(
    batch_size: int,
    num_heads: int,
    seq_len: int,
    sparsity: float,
    device: torch.device | str,
) -> torch.Tensor:
    """
    ç”Ÿæˆ 128x128 block maskï¼Œæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
    1. æ€»ä½“ sparsity ç²¾ç¡®ç­‰äºç›®æ ‡å€¼ï¼ˆå¼ºçº¦æŸï¼‰
       - sparsity = è¢«è·³è¿‡çš„æ¯”ä¾‹ï¼Œdensity = 1 - sparsity = è¢«è®¡ç®—çš„æ¯”ä¾‹
    2. ç›¸é‚»è¡Œå¯¹ (2k, 2k+1) å…±äº«ç›¸åŒçš„ mask pattern
    3. æ¯ pair è‡³å°‘æœ‰ 1 ä¸ª True
    4. ä¸åŒ pair çš„ k å€¼å¯ä»¥ä¸åŒ
    """
    if not (0.0 <= sparsity < 1.0):
        raise ValueError(f"sparsity must be in [0, 1), got {sparsity}")

    num_blocks = math.ceil(seq_len / BLOCK_USER)
    # num_q_pairs = (num_blocks + 1) // 2  # Q æ–¹å‘çš„è¡Œå¯¹æ•°é‡
    num_q_pairs = num_blocks

    # density = 1 - sparsityï¼ˆè¢«è®¡ç®—çš„æ¯”ä¾‹ï¼‰
    # sparsity 80% â†’ density 20% â†’ åªè®¡ç®— 20% çš„ blocks
    density = 1.0 - sparsity

    # ç›®æ ‡æ€» True æ•°é‡ï¼ˆå¼ºçº¦æŸï¼‰
    total_elements = num_q_pairs * num_blocks
    total_true_target = int(density * total_elements)
    total_true_target = max(total_true_target, num_q_pairs)  # æ¯ pair è‡³å°‘ 1 ä¸ª
    total_true_target = min(total_true_target, total_elements)  # ä¸è¶…ä¸Šé™

    # ç”Ÿæˆéšæœºåˆ†æ•°
    scores = torch.rand((batch_size, num_heads, num_q_pairs, num_blocks), device=device)

    # Step 1: å…ˆä¸ºæ¯ pair é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ 1 ä¸ªä½ç½®ï¼ˆä¿è¯æ¯ pair è‡³å°‘æœ‰ 1 ä¸ªï¼‰
    _, first_indices = scores.max(dim=-1, keepdim=True)  # (B, H, num_q_pairs, 1)
    pair_mask = torch.zeros_like(scores, dtype=torch.bool)
    pair_mask.scatter_(3, first_indices, True)

    # Step 2: å‰©ä½™éœ€è¦é€‰æ‹©çš„æ•°é‡
    remaining = total_true_target - num_q_pairs

    if remaining > 0:
        # å°†å·²é€‰ä¸­çš„ä½ç½®åˆ†æ•°è®¾ä¸º -infï¼Œé¿å…é‡å¤é€‰æ‹©
        scores_masked = scores.clone()
        scores_masked.scatter_(3, first_indices, float("-inf"))

        # å±•å¹³å¹¶é€‰æ‹© top-remaining
        scores_flat = scores_masked.view(batch_size, num_heads, -1)
        _, top_indices = torch.topk(scores_flat, remaining, dim=-1)

        # æ›´æ–° mask
        pair_mask_flat = pair_mask.view(batch_size, num_heads, -1)
        pair_mask_flat.scatter_(2, top_indices, True)
        pair_mask = pair_mask_flat.view(batch_size, num_heads, num_q_pairs, num_blocks)

    # æ‰©å±•åˆ° (batch_size, num_heads, num_blocks, num_blocks)
    # æ¯å¯¹è¡Œé‡å¤ 2 æ¬¡ï¼ˆç¬¬ 2k è¡Œå’Œç¬¬ 2k+1 è¡Œç›¸åŒï¼‰
    # block_mask = pair_mask.repeat_interleave(2, dim=2)

    # å¦‚æœ num_blocks æ˜¯å¥‡æ•°ï¼Œæˆªæ–­å¤šä½™çš„è¡Œ
    block_mask = pair_mask[:, :, :num_blocks, :]

    return block_mask


# def merge_block_mask_128_to_256_random(block_mask_128: torch.Tensor) -> torch.Tensor:
#     """å°† 128x128 block mask éšæœºåˆå¹¶æˆ 256x128 ç²—ç²’åº¦ maskã€‚"""
#     if block_mask_128.ndim != 4:
#         raise ValueError(f"block_mask_128 must be 4D, got shape {block_mask_128.shape}")

#     batch_size, num_heads, num_q_blocks, num_k_blocks = block_mask_128.shape
#     if num_q_blocks % 2 == 1:
#         pad_row = block_mask_128[:, :, -1:, :]
#         block_mask_128 = torch.cat([block_mask_128, pad_row], dim=2)
#         num_q_blocks += 1

#     paired = block_mask_128.view(batch_size, num_heads, num_q_blocks // 2, 2, num_k_blocks)
#     selector = torch.rand(
#         (batch_size, num_heads, num_q_blocks // 2, 1),
#         device=block_mask_128.device,
#     ) < 0.5
#     merged = torch.where(selector, paired[:, :, :, 0, :], paired[:, :, :, 1, :])
#     return merged.to(torch.uint8)


def generate_upsampled_mask_mod(
    binary_mask: torch.Tensor, block_size: int = 128
) -> Callable:
    """
    ä» binary block mask ç”Ÿæˆ mask_mod å‡½æ•°ç”¨äº create_block_mask
    
    Args:
        binary_mask: (B, H, num_q_blocks, num_k_blocks) çš„ 0-1 mask
        block_size: block å¤§å°
    
    Returns:
        mask_mod: ç”¨äº create_block_mask çš„å‡½æ•°
    """
    def upsampled_mask_mod(b, h, q_idx, kv_idx):
        downsampled_q_idx = q_idx // block_size
        downsampled_kv_idx = kv_idx // block_size
        return binary_mask[b, h, downsampled_q_idx, downsampled_kv_idx]
    return upsampled_mask_mod


def create_block_mask_from_binary(
    binary_mask: torch.Tensor,
    seq_len: int,
    q_block_size: int,
    k_block_size: int,
    mask_mod: Callable,
) -> BlockMask:
    """
    ä» binary block mask ç›´æ¥æ„é€  FlexAttention çš„ BlockMaskï¼Œç»•è¿‡ create_block_mask çš„ O(seqÂ²) å†…å­˜é—®é¢˜ã€‚
    
    Args:
        binary_mask: (B, H, num_q_blocks, num_k_blocks) çš„ 0-1 mask
        seq_len: åºåˆ—é•¿åº¦
        q_block_size: Q æ–¹å‘ block å¤§å°
        k_block_size: K æ–¹å‘ block å¤§å°
        mask_mod: FlexAttention éœ€è¦çš„ mask_mod å‡½æ•°
    
    Returns:
        BlockMask: FlexAttention çš„ BlockMask å¯¹è±¡
    """
    B, H, num_q_blocks, num_k_blocks = binary_mask.shape
    device = binary_mask.device
    
    # kv_num_blocks: æ¯ä¸ª Q block éœ€è¦è®¡ç®—å¤šå°‘ä¸ª KV blocks
    kv_num_blocks = binary_mask.sum(dim=-1).to(torch.int32)  # (B, H, num_q_blocks)
    
    # kv_indices: éœ€è¦è®¡ç®—çš„ KV block ç´¢å¼•
    max_kv_per_q = kv_num_blocks.max().item()
    if max_kv_per_q == 0:
        max_kv_per_q = 1  # è‡³å°‘åˆ†é… 1 ä¸ªä½ç½®
    
    kv_indices = torch.zeros(B, H, num_q_blocks, max_kv_per_q, dtype=torch.int32, device=device)
    
    # å¡«å…… kv_indicesï¼ˆå‘é‡åŒ–å®ç°ï¼‰
    for b in range(B):
        for h in range(H):
            for q in range(num_q_blocks):
                mask_row = binary_mask[b, h, q]  # (num_k_blocks,)
                indices = torch.nonzero(mask_row, as_tuple=False).squeeze(-1)  # éé›¶ä½ç½®
                if indices.numel() > 0:
                    kv_indices[b, h, q, :indices.numel()] = indices.to(torch.int32)
    
    # ä½¿ç”¨ FlexAttention ä½¿ç”¨çš„ BLOCK_SIZEï¼ˆ128x128ï¼‰
    flex_block_size = 128
    
    return BlockMask(
        seq_lengths=(seq_len, seq_len),
        kv_num_blocks=kv_num_blocks,
        kv_indices=kv_indices,
        full_kv_num_blocks=None,
        full_kv_indices=None,
        q_num_blocks=None,
        q_indices=None,
        full_q_num_blocks=None,
        full_q_indices=None,
        BLOCK_SIZE=(flex_block_size, flex_block_size),
        mask_mod=mask_mod,
    )


def create_cute_block_sparse_mask_mod(block_size: int = 128):
    """
    åˆ›å»º CuTe JIT æ ¼å¼çš„ block sparse mask_mod å‡½æ•°(ç”¨äº fine mode)
    
    è¿™ä¸ªå‡½æ•°ä» aux_tensors[0] è¯»å– 128x128 ç²’åº¦çš„ block maskï¼Œ
    å¹¶åœ¨ element level åšç²¾ç¡®çš„ maskingã€‚
    
    Args:
        block_size: block å¤§å°(é»˜è®¤ 128)
    
    Returns:
        mask_mod: CuTe JIT æ ¼å¼çš„ mask_mod å‡½æ•°
    """
    if not FA4_AVAILABLE:
        raise RuntimeError("FA4 not available, cannot create CuTe mask_mod")
    
    # @fast_sampling
    @cute.jit
    def cute_block_sparse_mask(
        batch: cute.TensorSSA,
        head: cute.TensorSSA,
        m_idx: cute.TensorSSA,
        n_idx: cute.TensorSSA,
        seqlen_info,
        aux_tensors: list,
    ) -> cute.TensorSSA:
        """
        CuTe JIT mask_mod å‡½æ•°ï¼Œä» aux_tensors[0] è¯»å– block mask
        
        aux_tensors[0]: (B, H, num_q_blocks, num_k_blocks) çš„ 0-1 mask
        """
        block_mask = aux_tensors[0]
        block_size_ssa = cute_utils.scalar_to_ssa(block_size, cutlass.Int32)
        
        # è®¡ç®— block ç´¢å¼•
        q_block_idx = m_idx // block_size_ssa
        k_block_idx = n_idx // block_size_ssa
        
        # ä» block mask ä¸­è¯»å–å€¼
        mask_value = cute_utils.scalar_to_ssa(
            block_mask[batch[0], head[0], q_block_idx[0], k_block_idx[0]], 
            cutlass.Int32
        )
        
        # è¿”å› True å¦‚æœ mask å€¼ä¸º 1
        return mask_value > cute_utils.scalar_to_ssa(0, cutlass.Int32)
    
    return cute_block_sparse_mask


def convert_block_mask_to_fa4_format(
    block_mask_flex: BlockMask,
) -> BlockSparseTensorsTorch:
    """
    å°† FlexAttention çš„ BlockMask è½¬æ¢ä¸º FA4 BlockSparseTensorsTorch æ ¼å¼
    
    Args:
        block_mask_flex: FlexAttention çš„ BlockMask å¯¹è±¡
    
    Returns:
        BlockSparseTensorsTorch: FA4 æ‰€éœ€çš„ block sparse tensors
    """
    (
        _seq_q,
        _seq_k,
        kv_mask_cnt,
        kv_mask_idx,
        full_kv_cnt,
        full_kv_idx,
        q_mask_cnt,
        q_mask_idx,
        full_q_cnt,
        full_q_idx,
        *_,
    ) = block_mask_flex.as_tuple()
    
    return BlockSparseTensorsTorch(
        mask_block_cnt=kv_mask_cnt,
        mask_block_idx=kv_mask_idx,
        full_block_cnt=full_kv_cnt,
        full_block_idx=full_kv_idx,
    )


def convert_binary_mask_to_fa4_format(
    block_mask_binary: torch.Tensor,
    q_stage: int = 1,
) -> BlockSparseTensorsTorch:
    """
    ç›´æ¥ä» binary block mask åˆ›å»º FA4 BlockSparseTensorsTorch æ ¼å¼
    ä¸éœ€è¦é€šè¿‡ FlexAttentionï¼Œé¿å…å¤§ seq_len æ—¶çš„ OOM
    
    å¯¹äº non-causal 128x128 block sparse (q_stage=1):
    - æ‰€æœ‰é€‰ä¸­çš„ block éƒ½æ˜¯ FULL blockï¼ˆä¸éœ€è¦ element-level maskï¼‰
    - full_block_cnt/idx åŒ…å«é€‰ä¸­çš„ block
    - mask_block_cnt/idx åº”ä¸ºç©º
    
    Args:
        block_mask_binary: (B, H, num_q_blocks, num_k_blocks) çš„ 0-1 mask
        q_stage: FA4 çš„ q_stage å‚æ•°ï¼ŒSM100 ä¸Š q_stage=1 è¡¨ç¤º128x128 ç²’åº¦ï¼Œç›®å‰å·²æ”¯æŒ 128x128 ç²’åº¦
    
    Returns:
        BlockSparseTensorsTorch: FA4 æ‰€éœ€çš„ block sparse tensors
    """
    B, H, num_q_blocks, num_k_blocks = block_mask_binary.shape
    device = block_mask_binary.device
    
    # FA4 æœŸæœ›çš„ q blocks æ•°é‡æ˜¯ ceil(num_q_blocks / q_stage)
    # éœ€è¦åˆå¹¶ç›¸é‚»çš„ q blocks
    if q_stage > 1:
        # å°†ç›¸é‚»çš„ q_stage ä¸ª q blocks åˆå¹¶(å– OR)
        # å…ˆ padding ä½¿å¾— num_q_blocks æ˜¯ q_stage çš„å€æ•°
        pad_q = (q_stage - num_q_blocks % q_stage) % q_stage
        if pad_q > 0:
            padding = torch.zeros(B, H, pad_q, num_k_blocks, dtype=block_mask_binary.dtype, device=device)
            block_mask_binary = torch.cat([block_mask_binary, padding], dim=2)
        
        # reshape å¹¶åˆå¹¶
        new_num_q_blocks = block_mask_binary.shape[2] // q_stage
        block_mask_binary = block_mask_binary.view(B, H, new_num_q_blocks, q_stage, num_k_blocks)
        # åˆå¹¶æ–¹å¼ï¼šå¦‚æœä»»ä½•ä¸€ä¸ªå­ block æœ‰ maskï¼Œåˆå¹¶åçš„ block å°±æœ‰ mask
        block_mask_binary = block_mask_binary.any(dim=3).to(torch.uint8)
    
    B, H, num_q_blocks_effective, num_k_blocks = block_mask_binary.shape
    
    # è®¡ç®—æ¯ä¸ª query block å¯¹åº”çš„æœ‰æ•ˆ key block æ•°é‡
    # å¯¹äº non-causalï¼Œæ‰€æœ‰é€‰ä¸­çš„ block éƒ½æ˜¯ FULL block
    # shape: (B, H, num_q_blocks_effective)
    full_block_cnt = block_mask_binary.sum(dim=-1).to(torch.int32)
    
    # åˆ›å»º full_block_idx: (B, H, num_q_blocks_effective, num_k_blocks)
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè·å–æ¯ä¸ª query block çš„æœ‰æ•ˆ key block ç´¢å¼•
    # å°†æœ‰æ•ˆçš„ k block ç´¢å¼•æ’åœ¨å‰é¢ï¼Œæ— æ•ˆçš„ä½ç½®å¡« 0
    positions = torch.arange(num_k_blocks, device=device).view(1, 1, 1, -1).expand(B, H, num_q_blocks_effective, -1)
    masked_positions = torch.where(
        block_mask_binary.bool(),
        positions.float(),
        torch.tensor(float('inf'), device=device)
    )
    sorted_indices = masked_positions.sort(dim=-1).values
    sorted_indices = torch.where(
        sorted_indices == float('inf'),
        torch.zeros_like(sorted_indices),
        sorted_indices
    )
    full_block_idx = sorted_indices.to(torch.int32)
    
    # å¯¹äº non-causal 128x128 block sparseï¼Œmask_block åº”ä¸ºç©º
    # ï¼ˆé™¤äº†å¯¹äºè¶…å‡º seqlen çš„ blockï¼Œå…¶ä½™block å†…éƒ¨ä¸éœ€è¦ element-level maskingï¼‰
    mask_block_cnt = torch.zeros_like(full_block_cnt)
    mask_block_idx = torch.zeros_like(full_block_idx)
    
    return BlockSparseTensorsTorch(
        mask_block_cnt=mask_block_cnt,
        mask_block_idx=mask_block_idx,
        full_block_cnt=full_block_cnt,
        full_block_idx=full_block_idx,
    )


def expand_block_mask(
    block_mask: torch.Tensor, 
    seq_len: int, 
    q_block_size: int = 128,
    k_block_size: int = 128,
) -> torch.Tensor:
    """
    å°† block_mask æ‰©å±•ä¸ºå®Œæ•´çš„ attention mask
    
    Args:
        block_mask: (B, H, num_q_blocks, num_k_blocks) çš„ 0-1 mask
        seq_len: åºåˆ—é•¿åº¦
        q_block_size: Q æ–¹å‘çš„ block å¤§å°
        k_block_size: K æ–¹å‘çš„ block å¤§å°
    
    Returns:
        expanded_mask: (B, H, seq_len, seq_len) çš„ attention mask
    """
    B, H, nb_q, nb_k = block_mask.shape
    expanded = block_mask.repeat_interleave(q_block_size, dim=2).repeat_interleave(
        k_block_size, dim=3
    )
    return expanded[:, :, :seq_len, :seq_len]


@nvtx.annotate("pytorch_reference.forward", color="green")
def pytorch_reference(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    mask: Optional[torch.Tensor] = None,
    sm_scale: Optional[float] = None,
) -> torch.Tensor:
    """PyTorch å‚è€ƒå®ç°"""
    if sm_scale is None:
        sm_scale = 1.0 / (q.shape[-1] ** 0.5)
    scores = torch.matmul(q, k.transpose(-2, -1)) * sm_scale
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attn = torch.softmax(scores, dim=-1)
    attn = torch.nan_to_num(attn, 0.0)
    out = torch.matmul(attn, v)
    return out


@nvtx.annotate("pytorch_reference_chunked.forward", color="green")
def pytorch_reference_chunked(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    block_mask: Optional[torch.Tensor] = None,
    q_block_size: int = 128,
    k_block_size: int = 128,
    sm_scale: Optional[float] = None,
    chunk_size: int = 512,
) -> torch.Tensor:
    """
    PyTorch å‚è€ƒå®ç° - åˆ†å—ç‰ˆæœ¬ï¼ŒèŠ‚çœå†…å­˜
    
    å…³é”®æ”¹è¿›ï¼šç›´æ¥æ¥å— block_maskï¼ŒåŠ¨æ€ç”Ÿæˆæ¯ä¸ª chunk çš„ mask sliceï¼Œ
    é¿å…åˆ›å»ºå®Œæ•´çš„ (B, H, seq, seq) expanded maskã€‚
    
    Args:
        q, k, v: (B, H, N, D) æ ¼å¼çš„è¾“å…¥
        block_mask: (B, H, num_q_blocks, num_k_blocks) çš„ block-level mask
        q_block_size: Q æ–¹å‘çš„ block å¤§å°
        k_block_size: K æ–¹å‘çš„ block å¤§å°
        sm_scale: softmax scale
        chunk_size: æ¯æ¬¡å¤„ç†çš„ Q åºåˆ—é•¿åº¦
    """
    if sm_scale is None:
        sm_scale = 1.0 / (q.shape[-1] ** 0.5)
    
    B, H, N, D = q.shape
    output = torch.zeros_like(q)
    
    for i in range(0, N, chunk_size):
        end_i = min(i + chunk_size, N)
        q_chunk = q[:, :, i:end_i, :]
        scores = torch.matmul(q_chunk, k.transpose(-2, -1)) * sm_scale  # (B, H, chunk, N)
        
        if block_mask is not None:
            # åŠ¨æ€ç”Ÿæˆå½“å‰ chunk çš„ maskï¼Œä¸å­˜å‚¨å®Œæ•´ mask
            chunk_len = end_i - i
            # è®¡ç®—å½“å‰ chunk æ¶‰åŠçš„ Q block èŒƒå›´
            q_block_start = i // q_block_size
            q_block_end = (end_i - 1) // q_block_size + 1
            
            # åªæ‰©å±•å½“å‰ chunk éœ€è¦çš„ mask éƒ¨åˆ†
            # block_mask: (B, H, num_q_blocks, num_k_blocks)
            chunk_block_mask = block_mask[:, :, q_block_start:q_block_end, :]  # (B, H, chunk_blocks, num_k_blocks)
            
            # æ‰©å±•åˆ° element level
            expanded_q = chunk_block_mask.repeat_interleave(q_block_size, dim=2)  # (B, H, chunk_blocks*q_block_size, num_k_blocks)
            expanded_kv = expanded_q.repeat_interleave(k_block_size, dim=3)  # (B, H, chunk_blocks*q_block_size, N_padded)
            
            # è£å‰ªåˆ°å®é™…éœ€è¦çš„å¤§å°
            local_start = i - q_block_start * q_block_size
            mask_chunk = expanded_kv[:, :, local_start:local_start + chunk_len, :N]
            
            scores = scores.masked_fill(mask_chunk == 0, float("-inf"))
            del chunk_block_mask, expanded_q, expanded_kv, mask_chunk
        
        attn = torch.softmax(scores, dim=-1)
        attn = torch.nan_to_num(attn, 0.0)
        output[:, :, i:end_i, :] = torch.matmul(attn, v)
        del scores, attn
    
    return output


@nvtx.annotate("fa4_sparse_attention.forward", color="blue")
def fa4_sparse_attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    block_sparse_tensors: BlockSparseTensorsTorch,
    m_block_size: int = 128,
    n_block_size: int = 128,
) -> torch.Tensor:
    """
    ä½¿ç”¨ FA4 cute interface è®¡ç®— block sparse attention
    
    Args:
        query: (B, H, S, D) æ ¼å¼
        key: (B, H, S, D) æ ¼å¼
        value: (B, H, S, D) æ ¼å¼
        block_sparse_tensors: FA4 block sparse tensors
        m_block_size: Q æ–¹å‘çš„ tile å¤§å° (å¯¹äº SM100, æœ‰æ•ˆç²’åº¦ = q_stage * m_block_size)
        n_block_size: K æ–¹å‘çš„ tile å¤§å°
    
    Returns:
        output: (B, H, S, D) æ ¼å¼
    """
    if not FA4_AVAILABLE:
        raise RuntimeError(f"FA4 cute interface not available: {FA4_IMPORT_ERROR}")
    
    # FA4 æœŸæœ›è¾“å…¥æ ¼å¼ä¸º (B, S, H, D)ï¼Œéœ€è¦ä» (B, H, S, D) è½¬æ¢
    B, H, S, D = query.shape
    q = query.transpose(1, 2)  # (B, S, H, D) only change layout
    k = key.transpose(1, 2)
    v = value.transpose(1, 2)
    
    with nvtx.annotate("fa4_fwd_kernel", color="yellow"):
        out, lse = _flash_attn_fwd(
            q=q,
            k=k,
            v=v,
            softmax_scale=None,  # è‡ªåŠ¨è®¡ç®—
            causal=False,
            m_block_size=m_block_size,
            n_block_size=n_block_size,
            mask_mod=None,  # ä¸ä½¿ç”¨ mask_modï¼Œå®Œå…¨ä¾èµ– block_sparse_tensors
            block_sparse_tensors=block_sparse_tensors,
            aux_tensors=None,
            return_lse=False,
        )
    
    return out.transpose(1, 2)


@nvtx.annotate("flex_sparse_attention.forward", color="green")
def flex_sparse_attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    block_mask: BlockMask,
    flex_attention_fn: Optional[Callable] = None,
) -> torch.Tensor:
    """ä½¿ç”¨ FlexAttention è®¡ç®— sparse attention"""
    with torch.cuda.device(query.device):
        if flex_attention_fn is not None:
            hidden_states = flex_attention_fn(query, key, value, block_mask=block_mask)
        else:
            hidden_states = flex_attention(query, key, value, block_mask=block_mask)
    return hidden_states


@nvtx.annotate("pytorch_sdpa.forward", color="red")
def pytorch_sdpa(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
) -> torch.Tensor:
    """PyTorch SDPA (dense attention)"""
    return F.scaled_dot_product_attention(query, key, value)


def validate_accuracy(
    ref_data: torch.Tensor,
    test_data: torch.Tensor,
    message: str = "",
    rtol: float = 1e-4,
    atol: float = 1e-2,
    verbose: bool = False,
) -> bool:
    """è¯¯å·®éªŒè¯å‡½æ•°"""
    mae_tol = rmse_tol = 1e-3
    ref_data_f32 = ref_data.to(torch.float32)
    test_data_f32 = test_data.to(torch.float32)
    abs_err = (ref_data_f32 - test_data_f32).abs()
    rel_err = abs_err / (ref_data_f32.abs() + 1e-12)
    
    metrics = {}
    metrics['max_abs_err'] = abs_err.max().item()
    metrics['mae'] = abs_err.mean().item()
    metrics['rmse'] = torch.sqrt((abs_err ** 2).mean()).item()
    metrics['max_rel_err'] = rel_err.max().item()
    metrics['mean_rel_err'] = rel_err.mean().item()
    
    ref_flat = ref_data_f32.flatten()
    test_flat = test_data_f32.flatten()
    metrics['cosine_sim'] = F.cosine_similarity(
        ref_flat.unsqueeze(0), test_flat.unsqueeze(0)
    ).item()
    
    signal_power = (ref_data_f32 ** 2).mean()
    noise_power = (abs_err ** 2).mean()
    metrics['snr_db'] = 10 * torch.log10(signal_power / (noise_power + 1e-12)).item()
    
    metrics['allclose'] = torch.allclose(ref_data_f32, test_data_f32, rtol=rtol, atol=atol)
    within_tol = (abs_err <= atol + rtol * ref_data_f32.abs()).sum().item()
    metrics['pass_rate'] = within_tol / ref_data_f32.numel() * 100
    
    validation_results = (
        (metrics['allclose'] and metrics['pass_rate'] >= 95) and
        metrics['max_abs_err'] < atol and
        metrics['mae'] < mae_tol and
        metrics['rmse'] < rmse_tol
    )
    
    print("ğŸ” " + ">" * 5 + f" {message}")
    if validation_results:
        print(colored("âœ… Accuracy Validation Passed", "green"))
    else:
        print(colored("âŒ Accuracy Validation Failed", "red"))
    print(f"  Absolute Errors (atol={atol}, mae_tol={mae_tol}, rmse_tol={rmse_tol}):")
    print(f"    Max:    {metrics['max_abs_err']:.6e}")
    print(f"    Mean:   {metrics['mae']:.6e}")
    print(f"    RMSE:   {metrics['rmse']:.6e}")
    if verbose:
        print(f"  Cosine Similarity: {metrics['cosine_sim']:.8f}")
        print(f"  SNR: {metrics['snr_db']:.2f} dB")
        print(f"  Pass Rate: {metrics['pass_rate']:.2f}%")
    print("")
    return validation_results


def run_single_test(
    batch_size: int,
    seq_len: int,
    num_heads: int,
    head_dim: int,
    block_size: int = 128,
    sparsity: float = 0.5,
    device: str = "cuda",
    run_accuracy: bool = True,
    run_performance: bool = True,
    # mode: str = "coarse",
    test_case_idx: int = 0,
) -> Optional[dict[str, str]]:
    """
    è¿è¡Œå•ä¸ªæµ‹è¯• case
    
    Args:
        mode: "coarse" - 256x128 ç²—ç²’åº¦(æ—  mask_mod)
              "fine" - 128x128 ç»†ç²’åº¦(ä½¿ç”¨ CuTe JIT mask_mod)
    """
    print("=" * 70)
    print(colored(
        f"Test Case {test_case_idx}: B={batch_size}, S={seq_len}, H={num_heads}, D={head_dim}",
        "cyan"
    ))
    # SM100 FA4 ç¡¬ç¼–ç é™åˆ¶ï¼š
    # - m_block_size å¿…é¡»æ˜¯ 128ï¼ˆTMEM 32x32 åŸå­æ“ä½œè¦æ±‚ï¼‰
    # - q_stage=2 æ˜¯ç¡¬ç¼–ç çš„ï¼ˆå½“ seqlen > m_block_sizeï¼‰
    # - æœ‰æ•ˆ Q ç²’åº¦ = q_stage * m_block_size = 2 * 128 = 256
    # - å› æ­¤ SM100 ä¸Š FA4 block sparse åªæ”¯æŒ 256x128 ç²’åº¦
    # 
    # fine mode åœ¨ SM100 ä¸Šä¸å¯ç”¨ï¼ˆm_block_size=64 ä¸æ”¯æŒï¼‰
    # ä¸¤ç§ mode ä½¿ç”¨ç›¸åŒçš„ 256x128 é…ç½®ï¼Œfine mode ä»…ä½œä¸ºæ ‡è®°ä¿ç•™
    # if mode == "fine":
    #     print(colored(
    #         "WARNING: Fine mode (128x128) not supported on SM100. "
    #         "Using coarse mode (256x128) instead.", "yellow"
    #     ))
    #     mode = "coarse"
    
    mode = 'fine'
    mode_desc = "fine (128x128)"
    print(f"Mode: {mode_desc}, Block Size: {block_size}, Sparsity: {sparsity * 100:.1f}%")
    print("=" * 70)
    
    # SM100 å”¯ä¸€æ”¯æŒçš„é…ç½®: 256x128
    # m_block_size=128, q_stage=2 -> æœ‰æ•ˆ Q ç²’åº¦ = 256
    q_block_size = 128
    k_block_size = 128
    m_block_size = 128
    n_block_size = 128

    # ç”Ÿæˆ 128x128 block maskï¼Œå†éšæœºåˆå¹¶ä¸º 256x128
    block_mask_128 = generate_block_mask_128(
        batch_size=batch_size,
        num_heads=num_heads,
        seq_len=seq_len,
        sparsity=sparsity,
        device=device,
    )
    # block_mask_binary = merge_block_mask_128_to_256_random(block_mask_128)
    block_mask_binary = block_mask_128
    
    mask_density = block_mask_binary.sum().item() / block_mask_binary.numel()
    print(f"Actual mask density: {mask_density * 100:.2f}%")
    print(f"Block mask shape: {block_mask_binary.shape}")
    
    # ç”Ÿæˆ QKV å¼ é‡ (B, H, S, D) æ ¼å¼
    qkv_shape = (batch_size, num_heads, seq_len, head_dim)
    query = torch.randn(qkv_shape, dtype=torch.bfloat16, device=device)
    key = torch.randn(qkv_shape, dtype=torch.bfloat16, device=device)
    value = torch.randn(qkv_shape, dtype=torch.bfloat16, device=device)
    
    # åˆ›å»º FlexAttention block mask - ä½¿ç”¨ _compile=True é¿å… materialize full mask
    block_mask_flex = None
    flex_mask_created = False
    
    # åˆ›å»º FlexAttention mask_modï¼ˆç”¨äº flex_attention è¿è¡Œæ—¶ï¼‰
    # if mode == "coarse":
    #     # Coarse mode: 256x128 ç²’åº¦
    #     def flex_mask_mod(b, h, q_idx, kv_idx):
    #         q_block_idx = q_idx // q_block_size
    #         k_block_idx = kv_idx // k_block_size
    #         return block_mask_binary[b, h, q_block_idx, k_block_idx].bool()
    # else:
        # Fine mode: 128x128 ç²’åº¦
    flex_mask_mod = generate_upsampled_mask_mod(block_mask_binary, block_size=block_size)
    
    try:
        # ä½¿ç”¨ _compile=True æ¥é¿å… materialize full mask (O(seqÂ²) å†…å­˜)
        # å¦‚æœ mask åœ¨æ‰€æœ‰ batch å’Œ head ä¸Šç›¸åŒï¼Œå¯ä»¥ç”¨ B=None, H=None æ¥ broadcast
        block_mask_flex = create_block_mask(
            flex_mask_mod,
            B=batch_size,
            H=num_heads,
            Q_LEN=seq_len,
            KV_LEN=seq_len,
            device=device,
            BLOCK_SIZE=(block_size, block_size),
            _compile=True,  # å…³é”®ï¼šå¯ç”¨ç¼–è¯‘æ¨¡å¼é¿å… full mask materialization
        )
        flex_mask_created = True
        print(f"FlexAttention block mask created successfully (_compile=True)")
    except Exception as e:
        print(colored(f"Warning: Failed to create FlexAttention block mask: {e}", "yellow"))
        import traceback
        traceback.print_exc()
        block_mask_flex = None
        flex_mask_created = False
    
    # è½¬æ¢ä¸º FA4 æ ¼å¼
    # ä¸¤ç§ mode éƒ½ä¸éœ€è¦ mask_modï¼Œå®Œå…¨ä¾èµ– block_sparse_tensors
    block_sparse_tensors = None
    
    if FA4_AVAILABLE:
        try:
            # å¯¹äº SM100ï¼ŒFA4 å†…éƒ¨ä¼šè‡ªåŠ¨è®¾ç½® q_stage=2ï¼ˆå½“ seqlen > m_block_sizeï¼‰
            # block_sparse_tensors çš„ num_q_blocks åº”è¯¥æ˜¯ä»¥æœ‰æ•ˆç²’åº¦è®¡ç®—çš„
            # æœ‰æ•ˆ Q ç²’åº¦ = q_stage * m_block_size
            # - Coarse: m_block_size=128, æœ‰æ•ˆ Q ç²’åº¦=256 -> mask æ˜¯ 256 ç²’åº¦ï¼Œq_stage=1 ç»™ convert
            # - Fine: m_block_size=64, æœ‰æ•ˆ Q ç²’åº¦=128 -> mask æ˜¯ 128 ç²’åº¦ï¼Œq_stage=1 ç»™ convert
            # æ³¨ï¼šq_stage å‚æ•°åœ¨ convert_binary_mask_to_fa4_format ä¸­åªç”¨äºåˆå¹¶ç›¸é‚» Q blocks
            # å½“ mask çš„ç²’åº¦å·²ç»å’Œæœ‰æ•ˆ Q ç²’åº¦åŒ¹é…æ—¶ï¼Œä¸éœ€è¦åˆå¹¶
            block_sparse_tensors = convert_binary_mask_to_fa4_format(
                block_mask_binary, q_stage=1  # mask ç²’åº¦å·²ç»åŒ¹é…æœ‰æ•ˆ Q ç²’åº¦ï¼Œä¸éœ€è¦åˆå¹¶
            )
        except Exception as e:
            print(colored(f"Warning: Failed to convert to FA4 format: {e}", "yellow"))
            import traceback
            traceback.print_exc()
    
    # === ç²¾åº¦æµ‹è¯• ===
    if run_accuracy:
        print(colored("\n--- Accuracy Test ---", "yellow"))
        
        # PyTorch Reference - ä½¿ç”¨ chunked ç‰ˆæœ¬ï¼ŒåŠ¨æ€ç”Ÿæˆ mask é¿å… OOM
        # å¯¹äºå¤§åºåˆ—ï¼ˆå¦‚ 130Kï¼‰ï¼Œå®Œæ•´ expanded_mask éœ€è¦çº¦ 1TBï¼Œä¼š OOM
        # chunked ç‰ˆæœ¬ç›´æ¥ä½¿ç”¨ block_maskï¼Œæ¯æ¬¡åªæ‰©å±•å½“å‰ chunk çš„ mask
        try:
            # ä¼˜å…ˆä½¿ç”¨ chunked ç‰ˆæœ¬ï¼Œæ›´èŠ‚çœå†…å­˜
            ref_output = pytorch_reference_chunked(
                query, key, value,
                block_mask=block_mask_binary,
                q_block_size=q_block_size,
                k_block_size=k_block_size,
                chunk_size=1024,  # æ¯æ¬¡å¤„ç† 1024 ä¸ª Q tokens
            )
        except Exception as e:
            print(colored(f"Error in PyTorch reference: {e}", "red"))
            import traceback
            traceback.print_exc()
            ref_output = None
        
        # FA4 Block Sparse
        if FA4_AVAILABLE and block_sparse_tensors is not None and ref_output is not None:
            try:
                print(query.shape)
                print(block_sparse_tensors.mask_block_cnt.shape)
                print(block_sparse_tensors.mask_block_idx.shape)
                fa4_output = fa4_sparse_attention(
                    query, key, value,
                    block_sparse_tensors=block_sparse_tensors,
                    m_block_size=m_block_size,
                    n_block_size=n_block_size,
                )
                validate_accuracy(
                    ref_output, fa4_output,
                    message=f"PyTorch Reference vs FA4 Block Sparse ({mode} mode)"
                )
            except Exception as e:
                print(colored(f"FA4 Error: {e}", "red"))
                import traceback
                traceback.print_exc()
        elif not FA4_AVAILABLE:
            print(colored("âš ï¸ FA4 not available, skipping FA4 accuracy test", "yellow"))


        # FlexAttention - ä½¿ç”¨ compiled ç‰ˆæœ¬é¿å… materialize full scores matrix
        if flex_mask_created and ref_output is not None:
            try:
                flex_output = flex_sparse_attention(
                    query, key, value, block_mask_flex, flex_attention_compiled
                )
                validate_accuracy(
                    ref_output, flex_output,
                    message="PyTorch Reference vs FlexAttention"
                )
            except Exception as e:
                print(colored(f"FlexAttention Error: {e}", "red"))
                import traceback
                traceback.print_exc()

    
    # === æ€§èƒ½æµ‹è¯• ===
    summary: Optional[dict[str, str]] = None
    if run_performance:
        print(colored("\n--- Performance Test ---", "yellow"))
        
        results = {}
        mode_label = f"FA4 Block Sparse ({mode})"
        
        # Warmup
        print(colored(f"Warming up ({WARMUP_ITERATIONS} iterations)...", "magenta"))
        for _ in range(WARMUP_ITERATIONS):
            _ = pytorch_sdpa(query, key, value)
            if flex_mask_created:
                try:
                    _ = flex_sparse_attention(
                        query, key, value, block_mask_flex, flex_attention_compiled
                    )
                except Exception:
                    pass
            if FA4_AVAILABLE and block_sparse_tensors is not None:
                try:
                    _ = fa4_sparse_attention(
                        query, key, value,
                        block_sparse_tensors=block_sparse_tensors,
                        m_block_size=m_block_size,
                        n_block_size=n_block_size,
                    )
                except Exception:
                    pass
        torch.cuda.synchronize()
        
        # PyTorch SDPA (dense)
        print(colored(
            f"Running PyTorch SDPA ({PERF_BENCHMARK_ITERATIONS} iterations)...",
            "magenta"
        ))
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        torch.cuda.synchronize()
        with nvtx.annotate("Perf.SDPA", color="red"):
            start_event.record()
            for _ in range(PERF_BENCHMARK_ITERATIONS):
                _ = pytorch_sdpa(query, key, value)
            end_event.record()
        torch.cuda.synchronize()
        results['PyTorch SDPA (dense)'] = (
            start_event.elapsed_time(end_event) / PERF_BENCHMARK_ITERATIONS
        )
        
        # FlexAttention
        if flex_mask_created:
            print(colored(
                f"Running FlexAttention ({PERF_BENCHMARK_ITERATIONS} iterations)...",
                "magenta"
            ))
            try:
                start_event = torch.cuda.Event(enable_timing=True)
                end_event = torch.cuda.Event(enable_timing=True)
                torch.cuda.synchronize()
                with nvtx.annotate("Perf.FlexAttn", color="green"):
                    start_event.record()
                    for _ in range(PERF_BENCHMARK_ITERATIONS):
                        _ = flex_sparse_attention(
                            query, key, value, block_mask_flex, flex_attention_compiled
                        )
                    end_event.record()
                torch.cuda.synchronize()
                results['FlexAttention (sparse)'] = (
                    start_event.elapsed_time(end_event) / PERF_BENCHMARK_ITERATIONS
                )
            except Exception as e:
                print(colored(f"FlexAttention benchmark error: {e}", "red"))
                results['FlexAttention (sparse)'] = float('nan')
        
        # FA4 Block Sparse
        if FA4_AVAILABLE and block_sparse_tensors is not None:
            print(colored(
                f"Running {mode_label} ({PERF_BENCHMARK_ITERATIONS} iterations)...",
                "magenta"
            ))
            try:
                start_event = torch.cuda.Event(enable_timing=True)
                end_event = torch.cuda.Event(enable_timing=True)
                torch.cuda.synchronize()
                with nvtx.annotate("Perf.FA4", color="blue"):
                    start_event.record()
                    for _ in range(PERF_BENCHMARK_ITERATIONS):
                        _ = fa4_sparse_attention(
                            query, key, value,
                            block_sparse_tensors=block_sparse_tensors,
                            m_block_size=m_block_size,
                            n_block_size=n_block_size,
                        )
                    end_event.record()
                torch.cuda.synchronize()
                results[mode_label] = (
                    start_event.elapsed_time(end_event) / PERF_BENCHMARK_ITERATIONS
                )
            except Exception as e:
                print(colored(f"FA4 benchmark error: {e}", "red"))
                import traceback
                traceback.print_exc()
                results[mode_label] = float('nan')
        elif not FA4_AVAILABLE:
            print(colored("âš ï¸ FA4 not available, skipping FA4 performance test", "yellow"))
        
        # æ‰“å°ç»“æœ
        print("\nğŸ”¥ Performance Results:")
        base_time = results.get('PyTorch SDPA (dense)', 1.0)
        for name, time in results.items():
            if math.isnan(time):
                print(f"  {name}: ERROR")
            else:
                speedup = base_time / time if time > 0 else 0
                print(f"  {name}: {time:.4f} ms (speedup vs SDPA: {speedup:.2f}x)")

        if base_time is not None and not math.isnan(base_time):
            summary = {
                "Sparsity": f"{sparsity * 100:.0f}%",
                "Shape": f"({batch_size},{seq_len},{num_heads},{head_dim})",
                "Mode": mode,
                "SDPA (ms)": f"{base_time:.4f}",
            }
            flex_time = results.get('FlexAttention (sparse)')
            if flex_time is not None and not math.isnan(flex_time):
                summary["FlexAttn (ms)"] = f"{flex_time:.4f}"
                summary["FlexAttn Speedup"] = f"{base_time / flex_time:.2f}x"
            fa4_time = results.get(mode_label)
            if fa4_time is not None and not math.isnan(fa4_time):
                summary["FA4 (ms)"] = f"{fa4_time:.4f}"
                summary["FA4 Speedup"] = f"{base_time / fa4_time:.2f}x"
    
    torch.cuda.empty_cache()
    print("")
    return summary


def main() -> None:
    parser = argparse.ArgumentParser(
        description="FA4 Block Sparse Attention Demo for B200 (Blackwell/SM100)"
    )
    parser.add_argument(
        "--device", type=str, default="cuda:0",
        help="Device to run on"
    )
    parser.add_argument(
        "--sparsity", type=float, default=0.5,
        help="Sparsity level (0-1, higher = more sparse)"
    )
    parser.add_argument(
        "--accuracy-only", action="store_true",
        help="Only run accuracy tests"
    )
    parser.add_argument(
        "--performance-only", action="store_true",
        help="Only run performance tests"
    )
    parser.add_argument(
        "--log", action="store_true",
        help="Enable logging to file"
    )
    parser.add_argument(
        "--block-size", type=int, default=128,
        help="Block size for sparse attention (default: 128)"
    )
    parser.add_argument(
        "--mode", type=str, default="coarse", choices=["coarse", "fine"],
        help="Sparse mode: 'coarse' (256x128, no mask_mod) or 'fine' (128x128 + CuTe mask_mod)"
    )
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    tee_logger = None
    if args.log:
        log_filename = datetime.now().strftime("fa4_demo_%Y%m%d_%H%M%S.log")
        tee_logger = TeeLogger(log_filename)
        sys.stdout = tee_logger
        print(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {log_filename}")
    
    device = torch.device(args.device)
    mode_desc = "coarse (256x128)" if args.mode == "coarse" else "fine (128x128 + mask_mod)"
    print(colored(f"Running FA4 Block Sparse Demo on device: {device}", "cyan"))
    print(f"Mode: {mode_desc}")
    print(f"FA4 Available: {FA4_AVAILABLE}")
    print(f"CUDA Compute Capability: {torch.cuda.get_device_capability()}")
    print(f"PyTorch Version: {torch.__version__}")
    print(f"CUDA Version: {torch.version.cuda}")
    print("")
    
    run_accuracy = not args.performance_only
    run_performance = not args.accuracy_only
    perf_results: list[dict[str, str]] = []
    
    # å®šä¹‰æµ‹è¯• case: (batch_size, seq_len, num_heads, head_dim)
    test_cases = [
        # head_dim = 64
        (8, 130560, 4, 64),      # Case 1: å¤§åºåˆ—é•¿åº¦
        (64, 65280, 2, 64),      # Case 2: å¤§ batch
        # head_dim = 128
        (2, 1590, 4, 128),       # Case 3: å°åºåˆ—
        (2, 8160, 4, 128),       # Case 4: ä¸­ç­‰åºåˆ—
    ]
    
    # æµ‹è¯•å¤šä¸ªç¨€ç–åº¦
    sparsity_levels = [0.1, 0.4, 0.8]  # 10%, 40%, 80%
    
    test_idx = 0
    for sparsity in sparsity_levels:
        print(colored(f"\n{'#' * 80}", "magenta"))
        print(colored(f"# Testing Sparsity: {sparsity * 100:.0f}%", "magenta"))
        print(colored(f"{'#' * 80}\n", "magenta"))
        
        for batch_size, seq_len, num_heads, head_dim in test_cases:
            test_idx += 1
            # ä½¿ç”¨ chunked referenceï¼Œæ‰€æœ‰ case éƒ½å¯ä»¥åšç²¾åº¦æµ‹è¯•
            case_run_accuracy = run_accuracy
            # print case and sparsity
            print(colored(f"Test Case {test_idx}: B={batch_size}, S={seq_len}, H={num_heads}, D={head_dim}, Sparsity: {sparsity * 100:.0f}%", "cyan"))
            try:
                summary = run_single_test(
                    batch_size=batch_size,
                    seq_len=seq_len,
                    num_heads=num_heads,
                    head_dim=head_dim,
                    block_size=args.block_size,
                    sparsity=sparsity,
                    device=args.device,
                    run_accuracy=case_run_accuracy,
                    run_performance=run_performance,
                    test_case_idx=test_idx,
                )
                if summary is not None:
                    perf_results.append(summary)
            except Exception as e:
                print(colored(f"Test case {test_idx} failed: {e}", "red"))
                import traceback
                traceback.print_exc()
                continue

    if run_performance and perf_results:
        print(colored("\n" + "=" * 100, "green"))
        print(colored("Performance Summary Table", "green", attrs=["bold"]))
        print(colored("=" * 100, "green"))
        df = pd.DataFrame(perf_results)
        pd.set_option("display.max_columns", None)
        pd.set_option("display.width", None)
        pd.set_option("display.colheader_justify", "center")
        print(df.to_string(index=False))
        print("")
    
    if tee_logger:
        tee_logger.close()
        sys.stdout = tee_logger.terminal


if __name__ == "__main__":
    main()
